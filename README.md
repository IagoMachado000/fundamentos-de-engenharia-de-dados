# Fundamentos de Engenharia de Dados
Anota√ß√µes sobre o curso Fundamentos de Engenharia de Dados https://www.datascienceacademy.com.br/course/fundamentos-de-engenharia-de-dados

Engenharia de dados √© a √°rea da tecnologia respons√°vel por **projetar, construir, manter e otimizar sistemas que coletam, armazenam, processam e disponibilizam dados** para an√°lise e uso por outras √°reas, como ci√™ncia de dados, BI (business intelligence) e engenharia de software.

### Em outras palavras:
O engenheiro de dados prepara os dados brutos ‚Äî muitas vezes desorganizados, incompletos ou espalhados em v√°rios lugares ‚Äî e transforma isso em algo **organizado, limpo, acess√≠vel e eficiente para an√°lise**.

---

### Principais responsabilidades:
- **Coletar dados** de diversas fontes (bancos de dados, APIs, arquivos, etc.)
- **Criar pipelines de dados (ETL/ELT)** ‚Äî processos que extraem, transformam e carregam dados para um destino final.
- **Modelar dados** para torn√°-los mais √∫teis e acess√≠veis.
- **Garantir a qualidade dos dados** (dados limpos, consistentes e sem duplicidades).
- **Trabalhar com grandes volumes de dados** (Big Data).
- **Construir e gerenciar data lakes e data warehouses**.
- **Automatizar processos de ingest√£o e transforma√ß√£o de dados**.
- **Implementar pol√≠ticas de seguran√ßa e governan√ßa de dados**.

---

### Tecnologias comuns na engenharia de dados:
- **Linguagens**: Python, SQL, Scala.
- **ETL/ELT**: Apache Airflow, dbt, Talend, Dataflow.
- **Big Data**: Apache Spark, Hadoop.
- **Bancos de dados**: PostgreSQL, MySQL, MongoDB, BigQuery, Snowflake, Redshift.
- **Armazenamento em nuvem**: AWS (S3, Glue), GCP (BigQuery, Cloud Storage), Azure.
- **Ferramentas de orquestra√ß√£o e monitoramento**.

---

### Diferen√ßa para outras √°reas:
| √Årea                | Foco principal                                           |
|---------------------|----------------------------------------------------------|
| **Engenharia de Dados** | Infraestrutura e organiza√ß√£o dos dados                 |
| **Ci√™ncia de Dados**     | An√°lise, predi√ß√£o e cria√ß√£o de modelos com os dados    |
| **BI (Business Intelligence)** | Visualiza√ß√£o de dados para tomada de decis√£o        |
| **Engenharia de Software** | Desenvolvimento de sistemas e aplicativos             |

---

### GUIA DE ESTUDO E APRENDIZAGEM DA DATA SCIENCE ACADEMY 
[E-book](./pdf/49-E-book%20DSA%20_Guia_De_Estudo_Aprendizagem.pdf)

### Bibliografia, Refer√™ncias e Links √öteis
[Links](./pdf/10-BibliografiaCap01.pdf)

---

### Pipeline de dados

Um **pipeline de dados** √© como uma "linha de montagem" que **pega dados brutos de uma ou mais fontes, processa esses dados em etapas definidas e entrega o resultado pronto para ser usado** ‚Äî geralmente em um banco de dados, data lake, data warehouse ou ferramenta de an√°lise.

---

### Analogia simples:
Imagine que os dados s√£o gr√£os de caf√©:

1. **Extra√ß√£o**: pegar os gr√£os da planta√ß√£o (dados brutos).
2. **Transforma√ß√£o**: torrar, moer, filtrar (limpar, organizar, juntar).
3. **Carga**: servir o caf√© na x√≠cara (enviar os dados para o destino final).

Esse processo cont√≠nuo de **extra√ß√£o, transforma√ß√£o e carga** √© conhecido como **ETL (Extract, Transform, Load)** ou **ELT** (quando a transforma√ß√£o vem depois da carga).

---

### Fases de um pipeline de dados:

1. **Extra√ß√£o (Extract)**  
   Captura dados de fontes diversas:
   - APIs
   - Bancos de dados
   - Arquivos CSV, Excel
   - Servi√ßos de terceiros (como Google Analytics, Facebook Ads)

2. **Transforma√ß√£o (Transform)**  
   Prepara os dados:
   - Limpeza (remover duplicatas, preencher valores ausentes)
   - Convers√£o de formatos
   - Jun√ß√£o de diferentes tabelas
   - Aplica√ß√£o de regras de neg√≥cio

3. **Carga (Load)**  
   Envia os dados para o destino:
   - Banco relacional
   - Data warehouse (BigQuery, Redshift, Snowflake)
   - Data lake

---

### Exemplos de ferramentas de pipeline:

- **Apache Airflow** (orquestra√ß√£o de tarefas e agendamentos)
- **dbt** (transforma√ß√µes SQL em data warehouse)
- **Luigi**, **Prefect**, **Kedro** (orquestra√ß√£o)
- **Talend**, **Informatica**, **AWS Glue**, **GCP Dataflow**

---

### Por que usar um pipeline?

- Automatiza o processo de movimenta√ß√£o e tratamento de dados.
- Garante qualidade e consist√™ncia.
- Torna o fluxo de dados escal√°vel e monitor√°vel.
- Facilita a atualiza√ß√£o de dados em tempo real ou agendada.

---

### Exemplo de um pipeline de dados

#### üß† Cen√°rio:
Temos um arquivo `usuarios.csv` com dados de usu√°rios. Vamos:

1. **Extrair** os dados do CSV.  
2. **Transformar**: remover linhas com e-mails inv√°lidos.  
3. **Carregar**: salvar os dados limpos em um banco SQLite.

---

#### üìÅ Exemplo do CSV (`usuarios.csv`):

```csv
id,nome,email
1,Ana,ana@email.com
2,Jo√£o,joao@email.com
3,Lucas,lucas@email
4,Marina,marina@email.com
```

> Note que o email do Lucas est√° inv√°lido (sem ".com").

---

#### üíª C√≥digo do pipeline (`pipeline_etl.py`):

```python
import pandas as pd
import sqlite3
import re

# EXTRA√á√ÉO
def extrair_dados(caminho_csv):
    return pd.read_csv(caminho_csv)

# TRANSFORMA√á√ÉO
def limpar_dados(df):
    # Remove e-mails inv√°lidos com regex simples
    regex_email = r"[^@]+@[^@]+\.[^@]+"
    df_filtrado = df[df['email'].apply(lambda x: re.match(regex_email, x) is not None)]
    return df_filtrado

# CARGA
def carregar_dados(df, nome_banco):
    conn = sqlite3.connect(nome_banco)
    df.to_sql('usuarios', conn, if_exists='replace', index=False)
    conn.close()

# EXECU√á√ÉO DO PIPELINE
def pipeline():
    print("Iniciando pipeline...")
    dados = extrair_dados('usuarios.csv')
    print("Extra√ß√£o conclu√≠da!")

    dados_limpos = limpar_dados(dados)
    print("Transforma√ß√£o conclu√≠da!")

    carregar_dados(dados_limpos, 'dados.db')
    print("Carga conclu√≠da! Dados salvos no banco 'dados.db'.")

if __name__ == "__main__":
    pipeline()
```

---

#### üì¶ Resultado:
- O pipeline cria um banco SQLite (`dados.db`) com uma tabela `usuarios` contendo **somente os registros v√°lidos**.
- O Lucas ser√° exclu√≠do por ter um e-mail inv√°lido.

---

### Componentes de um pipeline de dados

#### ‚úÖ **1. Origem (Source)**  
S√£o os **locais onde os dados se encontram antes de serem processados**.

üîπ **Componentes**:
- Bancos de dados relacionais (MySQL, PostgreSQL, SQL Server)
- Bancos NoSQL (MongoDB, Firebase, Cassandra)
- APIs (Google Ads, Facebook, Stripe, etc.)
- Arquivos (CSV, Excel, JSON, XML)
- Sistemas legados (ERP, CRM, etc.)
- Dados em tempo real (Kafka, IoT)

---

#### ‚öôÔ∏è **2. Processamento (Processing)**  
√â o **cora√ß√£o do pipeline**, onde os dados s√£o extra√≠dos, tratados, organizados e preparados para o uso.

üîπ **Componentes**:

##### A) **Extra√ß√£o (Extract)**  
- Captura os dados da origem.

##### B) **Transforma√ß√£o (Transform)**  
- Limpeza, normaliza√ß√£o, enriquecimento e padroniza√ß√£o dos dados.

##### C) **Orquestra√ß√£o e Agendamento**
- Coordena a ordem e o momento em que cada tarefa roda.
- Ferramentas: Apache Airflow, Prefect, Dagster

##### D) **Valida√ß√£o e Qualidade de Dados**
- Verifica se os dados est√£o consistentes e corretos.
- Regras de neg√≥cio, alertas e checagens.

##### E) **Monitoramento e Logs**
- Acompanha o status das execu√ß√µes e detecta falhas.

---

#### üì¶ **3. Destino (Target)**  
√â onde os dados **processados s√£o armazenados e ficam prontos para uso** por sistemas, BI, machine learning, etc.

üîπ **Componentes**:
- Data Warehouses (BigQuery, Redshift, Snowflake)
- Data Lakes (AWS S3, Azure Data Lake)
- Bancos SQL (PostgreSQL, MySQL)
- NoSQL (MongoDB, Elasticsearch)
- Dashboards de BI (Power BI, Looker, Tableau)
- Modelos de Machine Learning

---

#### üß† Visual Resumido:

```plaintext
[ ORIGEM ]
    ‚Üì
[ EXTRA√á√ÉO ]
    ‚Üì
[ TRANSFORMA√á√ÉO ]
    ‚Üì
[ CARGA ]
    ‚Üì
[ DESTINO ]
    ‚Üì
[ CONSUMO (dashboards, an√°lises, ML, etc.) ]
```

Se quiser, posso montar um fluxograma visual com esses blocos para facilitar ainda mais. Deseja isso?