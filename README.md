# Fundamentos de Engenharia de Dados
AnotaÃ§Ãµes sobre o curso Fundamentos de Engenharia de Dados https://www.datascienceacademy.com.br/course/fundamentos-de-engenharia-de-dados

Engenharia de dados Ã© a Ã¡rea da tecnologia responsÃ¡vel por **projetar, construir, manter e otimizar sistemas que coletam, armazenam, processam e disponibilizam dados** para anÃ¡lise e uso por outras Ã¡reas, como ciÃªncia de dados, BI (business intelligence) e engenharia de software.

### Em outras palavras:
O engenheiro de dados prepara os dados brutos â€” muitas vezes desorganizados, incompletos ou espalhados em vÃ¡rios lugares â€” e transforma isso em algo **organizado, limpo, acessÃ­vel e eficiente para anÃ¡lise**.

---

### Principais responsabilidades:
- **Coletar dados** de diversas fontes (bancos de dados, APIs, arquivos, etc.)
- **Criar pipelines de dados (ETL/ELT)** â€” processos que extraem, transformam e carregam dados para um destino final.
- **Modelar dados** para tornÃ¡-los mais Ãºteis e acessÃ­veis.
- **Garantir a qualidade dos dados** (dados limpos, consistentes e sem duplicidades).
- **Trabalhar com grandes volumes de dados** (Big Data).
- **Construir e gerenciar data lakes e data warehouses**.
- **Automatizar processos de ingestÃ£o e transformaÃ§Ã£o de dados**.
- **Implementar polÃ­ticas de seguranÃ§a e governanÃ§a de dados**.

---

### Tecnologias comuns na engenharia de dados:
- **Linguagens**: Python, SQL, Scala.
- **ETL/ELT**: Apache Airflow, dbt, Talend, Dataflow.
- **Big Data**: Apache Spark, Hadoop.
- **Bancos de dados**: PostgreSQL, MySQL, MongoDB, BigQuery, Snowflake, Redshift.
- **Armazenamento em nuvem**: AWS (S3, Glue), GCP (BigQuery, Cloud Storage), Azure.
- **Ferramentas de orquestraÃ§Ã£o e monitoramento**.

---

### DiferenÃ§a para outras Ã¡reas:
| Ãrea                | Foco principal                                           |
|---------------------|----------------------------------------------------------|
| **Engenharia de Dados** | Infraestrutura e organizaÃ§Ã£o dos dados                 |
| **CiÃªncia de Dados**     | AnÃ¡lise, prediÃ§Ã£o e criaÃ§Ã£o de modelos com os dados    |
| **BI (Business Intelligence)** | VisualizaÃ§Ã£o de dados para tomada de decisÃ£o        |
| **Engenharia de Software** | Desenvolvimento de sistemas e aplicativos             |

---

### GUIA DE ESTUDO E APRENDIZAGEM DA DATA SCIENCE ACADEMY 
[E-book](./pdf/49-E-book%20DSA%20_Guia_De_Estudo_Aprendizagem.pdf)

### Bibliografia, ReferÃªncias e Links Ãšteis
[Links](./pdf/10-BibliografiaCap01.pdf)

---

### Pipeline de dados

Um **pipeline de dados** Ã© como uma "linha de montagem" que **pega dados brutos de uma ou mais fontes, processa esses dados em etapas definidas e entrega o resultado pronto para ser usado** â€” geralmente em um banco de dados, data lake, data warehouse ou ferramenta de anÃ¡lise.

---

### Analogia simples:
Imagine que os dados sÃ£o grÃ£os de cafÃ©:

1. **ExtraÃ§Ã£o**: pegar os grÃ£os da plantaÃ§Ã£o (dados brutos).
2. **TransformaÃ§Ã£o**: torrar, moer, filtrar (limpar, organizar, juntar).
3. **Carga**: servir o cafÃ© na xÃ­cara (enviar os dados para o destino final).

Esse processo contÃ­nuo de **extraÃ§Ã£o, transformaÃ§Ã£o e carga** Ã© conhecido como **ETL (Extract, Transform, Load)** ou **ELT** (quando a transformaÃ§Ã£o vem depois da carga).

---

### Fases de um pipeline de dados:

1. **ExtraÃ§Ã£o (Extract)**  
   Captura dados de fontes diversas:
   - APIs
   - Bancos de dados
   - Arquivos CSV, Excel
   - ServiÃ§os de terceiros (como Google Analytics, Facebook Ads)

2. **TransformaÃ§Ã£o (Transform)**  
   Prepara os dados:
   - Limpeza (remover duplicatas, preencher valores ausentes)
   - ConversÃ£o de formatos
   - JunÃ§Ã£o de diferentes tabelas
   - AplicaÃ§Ã£o de regras de negÃ³cio

3. **Carga (Load)**  
   Envia os dados para o destino:
   - Banco relacional
   - Data warehouse (BigQuery, Redshift, Snowflake)
   - Data lake

---

### Exemplos de ferramentas de pipeline:

- **Apache Airflow** (orquestraÃ§Ã£o de tarefas e agendamentos)
- **dbt** (transformaÃ§Ãµes SQL em data warehouse)
- **Luigi**, **Prefect**, **Kedro** (orquestraÃ§Ã£o)
- **Talend**, **Informatica**, **AWS Glue**, **GCP Dataflow**

---

### Por que usar um pipeline?

- Automatiza o processo de movimentaÃ§Ã£o e tratamento de dados.
- Garante qualidade e consistÃªncia.
- Torna o fluxo de dados escalÃ¡vel e monitorÃ¡vel.
- Facilita a atualizaÃ§Ã£o de dados em tempo real ou agendada.

---

### Exemplo de um pipeline de dados

#### ğŸ§  CenÃ¡rio:
Temos um arquivo `usuarios.csv` com dados de usuÃ¡rios. Vamos:

1. **Extrair** os dados do CSV.  
2. **Transformar**: remover linhas com e-mails invÃ¡lidos.  
3. **Carregar**: salvar os dados limpos em um banco SQLite.

---

#### ğŸ“ Exemplo do CSV (`usuarios.csv`):

```csv
id,nome,email
1,Ana,ana@email.com
2,JoÃ£o,joao@email.com
3,Lucas,lucas@email
4,Marina,marina@email.com
```

> Note que o email do Lucas estÃ¡ invÃ¡lido (sem ".com").

---

#### ğŸ’» CÃ³digo do pipeline (`pipeline_etl.py`):

```python
import pandas as pd
import sqlite3
import re

# EXTRAÃ‡ÃƒO
def extrair_dados(caminho_csv):
    return pd.read_csv(caminho_csv)

# TRANSFORMAÃ‡ÃƒO
def limpar_dados(df):
    # Remove e-mails invÃ¡lidos com regex simples
    regex_email = r"[^@]+@[^@]+\.[^@]+"
    df_filtrado = df[df['email'].apply(lambda x: re.match(regex_email, x) is not None)]
    return df_filtrado

# CARGA
def carregar_dados(df, nome_banco):
    conn = sqlite3.connect(nome_banco)
    df.to_sql('usuarios', conn, if_exists='replace', index=False)
    conn.close()

# EXECUÃ‡ÃƒO DO PIPELINE
def pipeline():
    print("Iniciando pipeline...")
    dados = extrair_dados('usuarios.csv')
    print("ExtraÃ§Ã£o concluÃ­da!")

    dados_limpos = limpar_dados(dados)
    print("TransformaÃ§Ã£o concluÃ­da!")

    carregar_dados(dados_limpos, 'dados.db')
    print("Carga concluÃ­da! Dados salvos no banco 'dados.db'.")

if __name__ == "__main__":
    pipeline()
```

---

#### ğŸ“¦ Resultado:
- O pipeline cria um banco SQLite (`dados.db`) com uma tabela `usuarios` contendo **somente os registros vÃ¡lidos**.
- O Lucas serÃ¡ excluÃ­do por ter um e-mail invÃ¡lido.

---

### Componentes de um pipeline de dados

#### âœ… **1. Origem (Source)**  
SÃ£o os **locais onde os dados se encontram antes de serem processados**.

ğŸ”¹ **Componentes**:
- Bancos de dados relacionais (MySQL, PostgreSQL, SQL Server)
- Bancos NoSQL (MongoDB, Firebase, Cassandra)
- APIs (Google Ads, Facebook, Stripe, etc.)
- Arquivos (CSV, Excel, JSON, XML)
- Sistemas legados (ERP, CRM, etc.)
- Dados em tempo real (Kafka, IoT)

---

#### âš™ï¸ **2. Processamento (Processing)**  
Ã‰ o **coraÃ§Ã£o do pipeline**, onde os dados sÃ£o extraÃ­dos, tratados, organizados e preparados para o uso.

ğŸ”¹ **Componentes**:

##### A) **ExtraÃ§Ã£o (Extract)**  
- Captura os dados da origem.

##### B) **TransformaÃ§Ã£o (Transform)**  
- Limpeza, normalizaÃ§Ã£o, enriquecimento e padronizaÃ§Ã£o dos dados.

##### C) **OrquestraÃ§Ã£o e Agendamento**
- Coordena a ordem e o momento em que cada tarefa roda.
- Ferramentas: Apache Airflow, Prefect, Dagster

##### D) **ValidaÃ§Ã£o e Qualidade de Dados**
- Verifica se os dados estÃ£o consistentes e corretos.
- Regras de negÃ³cio, alertas e checagens.

##### E) **Monitoramento e Logs**
- Acompanha o status das execuÃ§Ãµes e detecta falhas.

---

#### ğŸ“¦ **3. Destino (Target)**  
Ã‰ onde os dados **processados sÃ£o armazenados e ficam prontos para uso** por sistemas, BI, machine learning, etc.

ğŸ”¹ **Componentes**:
- Data Warehouses (BigQuery, Redshift, Snowflake)
- Data Lakes (AWS S3, Azure Data Lake)
- Bancos SQL (PostgreSQL, MySQL)
- NoSQL (MongoDB, Elasticsearch)
- Dashboards de BI (Power BI, Looker, Tableau)
- Modelos de Machine Learning

---

#### ğŸ§  Visual Resumido:

```plaintext
[ ORIGEM ]
    â†“
[ EXTRAÃ‡ÃƒO ]
    â†“
[ TRANSFORMAÃ‡ÃƒO ]
    â†“
[ CARGA ]
    â†“
[ DESTINO ]
    â†“
[ CONSUMO (dashboards, anÃ¡lises, ML, etc.) ]
```

---

### Pipeline de dados x pipeline ETL

Pipeline de dados e pipeline ETL **nem sempre sÃ£o a mesma coisa**, mas muitas vezes sÃ£o usados como **sinÃ´nimos** â€” especialmente em contextos mais simples.

Vamos ver a diferenÃ§a com clareza:

---

#### âœ… **Pipeline ETL** (Extract, Transform, Load)

Ã‰ um tipo especÃ­fico de pipeline de dados que segue **trÃªs etapas principais**:

1. **Extract (ExtraÃ§Ã£o)** â€“ pega dados brutos de uma ou mais fontes.  
2. **Transform (TransformaÃ§Ã£o)** â€“ limpa, trata e padroniza os dados.  
3. **Load (Carga)** â€“ envia os dados para um destino (ex: data warehouse).

ğŸ”¹ Muito usado quando o foco estÃ¡ em **mover e preparar dados para anÃ¡lises** ou BI.

---

#### ğŸ”„ **Pipeline de Dados** (Data Pipeline)

Ã‰ um termo **mais genÃ©rico e abrangente**.

Pode incluir:

- Pipelines ETL (ou ELT)
- Pipelines de streaming (dados em tempo real)
- Pipelines de machine learning (com ingestÃ£o, treino de modelo, deploy)
- Pipelines de replicaÃ§Ã£o de dados
- Pipelines de integraÃ§Ã£o contÃ­nua com dados

Ou seja, **todo pipeline ETL Ã© um pipeline de dados**, mas **nem todo pipeline de dados Ã© ETL**.

---

#### ğŸ’¡ Exemplo de diferenÃ§a:

- **Pipeline ETL**: Extrai dados do MySQL, transforma em pandas, carrega no BigQuery.
- **Pipeline de streaming**: Usa Kafka + Spark para processar dados de sensores em tempo real.
- **Pipeline de ML**: Coleta dados, transforma, treina modelo, gera previsÃµes automaticamente.

---

#### ğŸ§  Resumindo:
| Termo              | AbrangÃªncia | Finalidade principal           |
|--------------------|-------------|--------------------------------|
| **Pipeline ETL**   | Mais especÃ­fico | MovimentaÃ§Ã£o + tratamento de dados |
| **Pipeline de Dados** | Mais amplo     | Qualquer fluxo automatizado de dados |

---

### Principais ferramentas para construir pipeline de dados

#### ğŸ” 1. TransformaÃ§Ã£o de Dados

#### O que Ã©:
SÃ£o ferramentas que **tratam, limpam, enriquecem, organizam e preparam os dados** para uso â€” geralmente apÃ³s a extraÃ§Ã£o e antes do carregamento final.

#### Tarefas comuns:
- Padronizar nomes de colunas
- Corrigir valores inconsistentes
- Juntar dados de diferentes fontes
- Agregar mÃ©tricas (ex: soma de vendas por dia)

#### Ferramentas populares:
| Ferramenta     | DescriÃ§Ã£o breve |
|----------------|------------------|
| **dbt (Data Build Tool)** | Transforma dados usando SQL diretamente no data warehouse. Ideal para times de analytics. |
| **Apache Spark** | Processa grandes volumes de dados em cluster (paralelo), com suporte a batch e streaming. |
| **Pandas (Python)** | Biblioteca poderosa para transformar dados tabulares em notebooks/scripts. Ã“tima para prototipaÃ§Ã£o. |
| **Apache Beam** | Framework de transformaÃ§Ã£o com suporte a batch e streaming. Roda em Dataflow (GCP), Spark, Flink, etc. |
| **Airbyte / Fivetran / Talend** | Algumas dessas ferramentas tambÃ©m permitem transformaÃ§Ãµes, alÃ©m da extraÃ§Ã£o/carga. |

---

#### â˜ï¸ 2. Armazenamento e Cloud Computing

#### O que Ã©:
SÃ£o os **locais onde os dados sÃ£o armazenados**, organizados e disponibilizados â€” muitas vezes em nuvem. TambÃ©m inclui serviÃ§os que **escalam automaticamente**, como clusters, servidores e bancos gerenciados.

#### SubdivisÃµes:
- **Data warehouses** (anÃ¡lises)
- **Data lakes** (armazenamento bruto)
- **Bancos relacionais/NoSQL**
- **Infraestrutura em nuvem (IaaS/PaaS)**

#### Ferramentas populares:
| Ferramenta     | DescriÃ§Ã£o breve |
|----------------|------------------|
| **Google BigQuery** | Data warehouse serverless da GCP, ideal para grandes volumes e consultas rÃ¡pidas. |
| **Amazon Redshift** | Data warehouse da AWS, otimizado para anÃ¡lises massivas. |
| **Snowflake** | Data warehouse multi-cloud, altamente escalÃ¡vel. |
| **AWS S3** | Armazena arquivos em nuvem (data lake). |
| **Azure Data Lake** | Equivalente ao S3 na Azure. |
| **Databricks** | Plataforma para engenharia e ciÃªncia de dados baseada em Spark. |
| **Google Cloud Platform / AWS / Azure** | Provedores cloud com serviÃ§os integrados de dados, computaÃ§Ã£o, seguranÃ§a, etc. |

---

#### âš¡ 3. Real-Time Analytics (AnÃ¡lise em Tempo Real)

#### O que Ã©:
SÃ£o ferramentas e plataformas voltadas para **ingestÃ£o, processamento e anÃ¡lise de dados em tempo real ou quase tempo real**. Ideal para sistemas que precisam de respostas imediatas (ex: detecÃ§Ã£o de fraudes, monitoramento de sensores, logs).

#### Ferramentas populares:
| Ferramenta     | DescriÃ§Ã£o breve |
|----------------|------------------|
| **Apache Kafka** | Sistema de mensageria distribuÃ­do, ideal para capturar e transmitir eventos em tempo real. |
| **Apache Flink** | Processamento de dados em streaming com baixa latÃªncia. |
| **Apache Spark Structured Streaming** | MÃ³dulo do Spark para trabalhar com dados em tempo real. |
| **Google Dataflow** | ServiÃ§o de stream/batch processing na GCP, baseado em Apache Beam. |
| **Kinesis (AWS)** | Equivalente ao Kafka na AWS, ideal para ingestÃ£o de dados em tempo real. |
| **ClickHouse** | Banco OLAP de alta performance, muito usado para analytics em tempo real. |

---

#### ğŸ’¡ Resumo visual da classificaÃ§Ã£o:

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TransformaÃ§Ã£o de Dados    â”‚  â† limpeza, preparo, junÃ§Ãµes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  dbt, Spark, Pandas, Beam  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Armazenamento e Cloud       â”‚  â† onde os dados sÃ£o guardados
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BigQuery, Redshift, S3,     â”‚
â”‚  Snowflake, Databricks       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Real-Time Analytics       â”‚  â† dados em tempo real
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kafka, Flink, Dataflow,   â”‚
â”‚  Spark Streaming, Kinesis  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### O processo de engenharia de dados

* A engenharia de dados fornece o suporte necessÃ¡rio para que o processo de CiÃªncia de Dados possa ser executado
* A Engenharia de Dados nÃ£o participa de todo o processo de CiÃªncia de Dados, mas sim das atividades onde deve haver gestÃ£o de dados
* Uma das primeiras etapas do processo Ã© a extraÃ§Ã£o de dados de uma ou mais fonts, de dados em batch ou dados em streaming
* A etapa seguinte envolve a preparaÃ§Ã£o dos dados com atividades de limpeza, transformaÃ§Ã£o, enriquecimento e seguranÃ§a de acesso
* O processo continua com o armazenamento dos dados no destino ou uso em tempo real 
* Mas o que seria o uso dos dados? AnÃ¡lise, grÃ¡ficos, dashboards, machine learning, IA ou qualquer outra tarefa de CiÃªncia de dados
* O pipeline de dados pode ser executado uma Ãºnica vez ou diversas vezes
* A execuÃ§Ã£o do pipeline requer monitoramento, seguranÃ§a, validaÃ§Ã£o e documentaÃ§Ã£o
* O pipeline de dados Ã© entÃ£o criado, documentado e automatizado
* E entÃ£o outro processo serÃ¡ iniciado para outro produto de dados da empresa
* Um Arquiteto de Dados Ã© o responsÃ¡vel pro projetar a soluÃ§Ã£o
* O trabalho de um Engenheiro de Dados envolve construir e mantes a soluÃ§Ã£o de dados

---

### Ciclo de vida da engenharia de dados

![Diagrama ciclo de vida da engenharia de dados](./img/ciclo-de-vida-engenhraria-de-dados.png)

#### Fonte de dados (batch e streaming)

As **fontes de dados** podem ser classificadas com base em **como os dados sÃ£o coletados e processados**, e os dois principais modos sÃ£o:

#### 1. **Batch (Lote)**

- **O que Ã©**: Os dados sÃ£o coletados, armazenados por um tempo e processados **em blocos (lotes)** em intervalos definidos (ex: a cada hora, dia, semana).
- **Quando usar**: Quando os dados nÃ£o precisam ser atualizados em tempo real e podem ser processados periodicamente.
- **Exemplos**:
  - RelatÃ³rios financeiros gerados toda madrugada.
  - ImportaÃ§Ã£o de dados de um CRM (como Salesforce) uma vez por dia.
  - ETL noturno que processa arquivos CSV enviados por e-mail.
- **Ferramentas comuns**: Apache Airflow, AWS Glue, Talend, dbt, Spark (modo batch).

##### âœ… Vantagens:
- Mais simples de implementar e manter.
- Maior performance para grandes volumes.

##### âŒ Desvantagens:
- Dados nÃ£o estÃ£o atualizados em tempo real.
- Pode haver latÃªncia alta (minutos a horas).

#### 2. **Streaming (Tempo real)**

- **O que Ã©**: Os dados sÃ£o processados **em tempo real ou quase em tempo real**, assim que sÃ£o gerados.
- **Quando usar**: Quando Ã© necessÃ¡rio agir rapidamente com base nos dados mais recentes.
- **Exemplos**:
  - DetecÃ§Ã£o de fraudes bancÃ¡rias com base em transaÃ§Ãµes em tempo real.
  - Leituras de sensores IoT transmitidas continuamente.
  - Cliques em anÃºncios ou navegaÃ§Ã£o de usuÃ¡rios em um site.
- **Ferramentas comuns**: Apache Kafka, Apache Flink, Spark Streaming, AWS Kinesis, Google Pub/Sub.

##### âœ… Vantagens:
- ReaÃ§Ãµes rÃ¡pidas a eventos.
- Ideal para sistemas crÃ­ticos ou interativos.

##### âŒ Desvantagens:
- Mais complexo de desenvolver e monitorar.
- Pode ter maior custo operacional.

#### 3. **Lambda Architecture (HÃ­brido: Batch + Streaming)**

- **O que Ã©**: Combina o melhor dos dois mundos: uma camada de batch para consistÃªncia e uma de streaming para baixa latÃªncia.
- **Exemplo**:
  - Dados de pedidos de e-commerce processados em tempo real para atualizaÃ§Ã£o de estoque, mas reprocessados Ã  noite em batch para gerar relatÃ³rios consolidados.

#### Comparativo rÃ¡pido:

| CaracterÃ­stica     | Batch                          | Streaming                       |
|--------------------|--------------------------------|----------------------------------|
| LatÃªncia            | Alta (minutos a horas)         | Baixa (milissegundos a segundos)|
| Volume              | Muito alto de uma vez          | ContÃ­nuo e constante             |
| Complexidade        | Menor                          | Maior                            |
| Casos ideais        | BI, relatÃ³rios, ETL            | Alertas, logs, sistemas online   |

---

#### IngestÃ£o de dados

**IngestÃ£o de dados** Ã© o processo de **coletar dados de uma ou mais fontes** e **trazÃª-los para dentro de um sistema de armazenamento ou processamento**, como um data lake, data warehouse, banco de dados ou pipeline.

##### ğŸ” Explicando de forma simples:

Imagine que as fontes de dados sÃ£o como **torneiras espalhadas pela empresa** (sites, bancos de dados, APIs, arquivos, sensores, etc.).  
A ingestÃ£o de dados Ã© o **encanamento que conecta essas torneiras ao seu reservatÃ³rio de dados**, garantindo que a Ã¡gua (dados) chegue limpa, no horÃ¡rio certo e no volume certo.

##### Tipos de IngestÃ£o de Dados:

1. **Batch (lote)**  
   - Dados sÃ£o coletados em grandes blocos, de tempos em tempos.  
   - Ex: Buscar um arquivo CSV por FTP toda noite Ã s 2h da manhÃ£.

2. **Streaming (tempo real)**  
   - Dados sÃ£o ingeridos continuamente, conforme sÃ£o gerados.  
   - Ex: Processar eventos do Kafka conforme chegam.

3. **Micro-batch**  
   - Um meio-termo: dados sÃ£o agrupados em pequenos lotes com pouca latÃªncia.  
   - Ex: Um job do Spark rodando a cada 30 segundos.

##### Fontes comuns de ingestÃ£o:

- Bancos de dados (MySQL, PostgreSQL, Oracle)
- APIs REST
- Arquivos CSV, Excel, JSON, Parquet
- Sistemas ERP ou CRM
- Fila de mensagens (Kafka, RabbitMQ)
- Sensores e dispositivos IoT
- ServiÃ§os de nuvem (Google Analytics, Facebook Ads, etc.)

##### Ferramentas populares para ingestÃ£o:

- **Batch**: Apache Nifi, Apache Sqoop, Talend, Airbyte  
- **Streaming**: Apache Kafka, Flink, Logstash, Kinesis  
- **Cloud**: AWS Glue, Google Cloud Dataflow, Azure Data Factory  

##### Exemplos prÃ¡ticos:

- Ingerir logs de acesso de um site para anÃ¡lise de trÃ¡fego.
- Puxar diariamente os leads do RD Station para um banco.
- Receber dados de temperatura de sensores industriais em tempo real.

##### Por que a ingestÃ£o Ã© tÃ£o importante?

Porque **sem ingestÃ£o, nÃ£o hÃ¡ dado para trabalhar**. Ã‰ o primeiro passo para:
- Criar dashboards.
- Treinar modelos de machine learning.
- Fazer anÃ¡lises estatÃ­sticas.
- Automatizar decisÃµes baseadas em dados.

---

#### TransformaÃ§Ã£o e Enriquecimento

Depois da **ingestÃ£o**, os dados raramente estÃ£o prontos para uso. Ã‰ aÃ­ que entram a **transformaÃ§Ã£o** e o **enriquecimento** dos dados â€” duas etapas fundamentais na engenharia de dados.

##### ğŸ”§ **TransformaÃ§Ã£o de Dados**

**Transformar dados** significa alterar sua estrutura ou formato para que fiquem **mais limpos, padronizados, Ãºteis e compatÃ­veis com os objetivos do negÃ³cio**.

##### Exemplos de transformaÃ§Ã£o:
- Converter datas para um Ãºnico formato (ex: `01/04/2025` â†’ `2025-04-01`)
- Trocar vÃ­rgulas por pontos em nÃºmeros decimais (`12,3` â†’ `12.3`)
- Remover espaÃ§os em branco ou caracteres especiais
- Agrupar dados (ex: somar vendas por mÃªs)
- Normalizar textos (`SÃ£o Paulo`, `sao paulo`, `SÃƒO PAULO` â†’ `Sao Paulo`)
- Criar colunas novas a partir de outras (ex: extrair o **ano** de uma data)

##### Ferramentas e tÃ©cnicas:
- SQL (com `SELECT`, `CASE`, `CAST`, `JOIN`, etc.)
- Linguagens como Python (pandas), Scala, R
- Ferramentas ETL: dbt, Apache Spark, Airflow, etc.

##### âœ¨ **Enriquecimento de Dados**

**Enriquecer dados** Ã© o processo de **adicionar novas informaÃ§Ãµes a um conjunto de dados existente** para tornÃ¡-lo mais completo, Ãºtil e valioso.

##### Exemplos de enriquecimento:
- Juntar dados de clientes com dados geogrÃ¡ficos (CEP â†’ cidade e estado)
- Incluir a cotaÃ§Ã£o do dÃ³lar em uma tabela de exportaÃ§Ãµes
- Associar um ID de produto com sua descriÃ§Ã£o e categoria
- Usar uma API externa para descobrir a localizaÃ§Ã£o exata de um IP

#### Fontes para enriquecimento:
- Outras tabelas internas (via `JOIN`)
- APIs externas (Google Maps, Receita Federal, redes sociais)
- CatÃ¡logos de dados pÃºblicos

##### Resumindo:

| Etapa           | O que faz                                 | Exemplo prÃ¡tico                            |
|----------------|--------------------------------------------|---------------------------------------------|
| **TransformaÃ§Ã£o** | Limpa e organiza os dados                 | Corrigir formataÃ§Ã£o de datas, normalizar texto |
| **Enriquecimento** | Adiciona novas informaÃ§Ãµes Ãºteis          | Trazer o nome do cliente a partir de um ID  |

Ambas as etapas sÃ£o parte do famoso processo **ETL (Extract, Transform, Load)**, e sÃ£o essenciais para garantir que os dados estejam **confiÃ¡veis, completos e prontos para anÃ¡lise** ou machine learning.

---

#### Carga e Uso dos Dados

Depois de **ingestÃ£o**, **transformaÃ§Ã£o** e **enriquecimento**, o ciclo Ã© fechado com as fases finais do pipeline de engenharia de dados: **Carga (Load)** e **Uso dos Dados**.

##### ğŸšš **Carga (Load)**

A **carga de dados** Ã© o processo de **salvar os dados transformados e enriquecidos no destino final**, que pode ser:

- Um **Data Lake** (armazenamento bruto em grande escala, ex: S3, Google Cloud Storage)
- Um **Data Warehouse** (estrutura otimizada para anÃ¡lises, ex: BigQuery, Redshift, Snowflake)
- Um **banco de dados relacional** (ex: PostgreSQL, MySQL, SQL Server)
- Uma ferramenta de **BI** (ex: Power BI, Tableau)
- Um sistema de terceiros (ex: CRM, ERP, API externa)

##### Tipos de carga:
- **Full Load (carga total)**: os dados antigos sÃ£o apagados e substituÃ­dos por uma versÃ£o atualizada.
- **Incremental Load (carga incremental)**: apenas os dados novos ou modificados sÃ£o adicionados/atualizados.

##### Exemplo:
ApÃ³s transformar uma planilha de vendas, os dados sÃ£o carregados para um banco PostgreSQL, onde os analistas poderÃ£o consultÃ¡-los.

##### ğŸ“Š **Uso dos Dados**

Depois da carga, os dados estÃ£o prontos para serem **consumidos**. Isso Ã© onde eles finalmente **geram valor para o negÃ³cio**.

##### Principais formas de uso:

1. **Business Intelligence (BI)**  
   - Ferramentas como Power BI, Tableau e Looker acessam os dados e criam dashboards, KPIs e relatÃ³rios interativos.
   - Ex: visualizar vendas por regiÃ£o e por perÃ­odo.

2. **Data Science / Machine Learning**  
   - Cientistas de dados usam os dados carregados para treinar modelos, prever comportamentos ou detectar anomalias.
   - Ex: prever churn de clientes ou demanda futura.

3. **APIs e produtos de dados**  
   - Os dados podem alimentar APIs internas ou produtos digitais (apps, sistemas, etc.).
   - Ex: uma API de recomendaÃ§Ã£o de produtos baseada em dados de comportamento.

4. **Tarefas operacionais / automaÃ§Ãµes**  
   - RobÃ´s, scripts ou sistemas usam os dados para tomar decisÃµes ou realizar aÃ§Ãµes automaticamente.
   - Ex: disparar um e-mail de alerta quando um estoque estiver abaixo do mÃ­nimo.

##### ğŸ”„ Onde isso entra no fluxo geral?

```text
[IngestÃ£o] â†’ [TransformaÃ§Ã£o] â†’ [Enriquecimento] â†’ [Carga] â†’ [Uso dos Dados]
```

Cada etapa Ã© dependente da anterior, e todas fazem parte do **pipeline de dados**.

#### Exemplo prÃ¡tico

**Mini pipeline de engenharia de dados**, simulando um cenÃ¡rio real simples â€” passo a passo â€” desde a ingestÃ£o atÃ© o uso.

##### ğŸ§ª **CenÃ¡rio fictÃ­cio: AnÃ¡lise de vendas de uma loja online**

VocÃª trabalha como engenheiro de dados e precisa entregar dados de vendas diÃ¡rias limpos e prontos para o time de BI gerar relatÃ³rios.

###### âœ… 1. **IngestÃ£o**

**Fonte de dados**:  
Um sistema da loja exporta diariamente um arquivo CSV com as vendas do dia, como este:

```
id,produto_id,data_venda,preco,quantidade
1,101,27/04/2025,49,90,1
2,102,27/04/2025,129.99,2
```

VocÃª cria um script que **lÃª esse arquivo todo dia Ã s 23h** e o envia para o pipeline.

Ferramentas possÃ­veis: Python (`pandas`), Apache Airflow (agendamento), Shell Script.

##### ğŸ”§ 2. **TransformaÃ§Ã£o**

VocÃª nota problemas:
- O campo `data_venda` estÃ¡ no formato errado.
- O campo `preco` vem com vÃ­rgula ao invÃ©s de ponto (ex: `49,90`).
- Precisa de uma nova coluna: **valor_total** (preÃ§o Ã— quantidade)

VocÃª transforma os dados assim:

```python
import pandas as pd

df = pd.read_csv("vendas.csv", sep=",")
df['data_venda'] = pd.to_datetime(df['data_venda'], dayfirst=True)
df['preco'] = df['preco'].astype(str).str.replace(',', '.').astype(float)
df['valor_total'] = df['preco'] * df['quantidade']
```

##### âœ¨ 3. **Enriquecimento**

VocÃª tem uma outra tabela de produtos:

```csv
produto_id,nome,categoria
101,Camisa Polo,Roupas
102,Smartwatch,EletrÃ´nicos
```

VocÃª faz um **JOIN** para adicionar nome e categoria do produto:

```python
produtos = pd.read_csv("produtos.csv")
df = df.merge(produtos, on='produto_id', how='left')
```

Agora seu dataframe tem:

```
id | produto_id | data_venda | preco | quantidade | valor_total | nome         | categoria
1  | 101        | 2025-04-27 | 49.90 | 1          | 49.90       | Camisa Polo  | Roupas
```

##### ğŸšš 4. **Carga**

VocÃª salva os dados prontos no seu **Data Warehouse** (ex: PostgreSQL ou BigQuery):

```python
from sqlalchemy import create_engine

engine = create_engine("postgresql://user:senha@host:porta/banco")
df.to_sql("vendas_processadas", engine, if_exists="append", index=False)
```

##### ğŸ“Š 5. **Uso dos Dados**

Agora, o time de BI conecta o Power BI diretamente na tabela `vendas_processadas`, e pode:

- Ver **vendas por categoria**
- Comparar **faturamento por perÃ­odo**
- Criar **grÃ¡ficos e dashboards**

##### ğŸ”„ Resumo visual:

```text
ğŸ“ CSV de vendas (raw)
 â†“ ingestÃ£o
ğŸ§¹ Limpeza + CÃ¡lculo de total
 â†“ transformaÃ§Ã£o
â• AdiÃ§Ã£o de nome e categoria do produto
 â†“ enriquecimento
ğŸ’¾ Salva no PostgreSQL
 â†“ carga
ğŸ“Š Usado no Power BI para relatÃ³rios
```

---

#### Armazenamento

Vamos entender **onde entra o armazenamento** no pipeline de dados e **quais tipos existem**, com foco na etapa de **carga** e posterior **uso dos dados**.

##### ğŸ—„ï¸ **Armazenamento no pipeline de dados**

O **armazenamento** Ã© onde os dados ficam **guardados em cada etapa** do processo â€” desde os brutos atÃ© os prontos para anÃ¡lise.

Ele pode ocorrer em **vÃ¡rias camadas diferentes**, dependendo da arquitetura adotada. Veja abaixo:

##### ğŸ§± Tipos de armazenamento no pipeline:

##### 1. **Raw Layer (Camada Bruta)**
- **O que Ã©**: onde os dados sÃ£o armazenados *logo apÃ³s a ingestÃ£o*, sem transformaÃ§Ã£o.
- **Usos**: backup, reprocessamento, auditoria.
- **Formato**: arquivos CSV, JSON, Parquet, Avro.
- **Exemplos**:
  - Amazon S3
  - Google Cloud Storage
  - Azure Blob Storage
  - HDFS (Hadoop)

##### 2. **Staging Layer (Camada IntermediÃ¡ria ou TemporÃ¡ria)**
- **O que Ã©**: onde os dados **transformados parcialmente** ficam antes do armazenamento final.
- **Usos**: etapa de preparaÃ§Ã£o, controle de qualidade, testes.
- **Formato**: tabelas temporÃ¡rias, arquivos organizados.
- **Exemplos**:
  - Tabelas staging no PostgreSQL, BigQuery, etc.
  - Pastas intermediÃ¡rias no S3

##### 3. **Curated / Trusted Layer (Camada Curada ou ConfiÃ¡vel)**
- **O que Ã©**: onde os dados **prontos para anÃ¡lise** sÃ£o armazenados.
- **Usos**: BI, relatÃ³rios, ML, dashboards.
- **Formato**: tabelas bem definidas, padronizadas.
- **Exemplos**:
  - Data Warehouse (Redshift, BigQuery, Snowflake, PostgreSQL)
  - Data Mart (subconjunto voltado para Ã¡reas especÃ­ficas)

##### 4. **Serving Layer (Camada de Consumo)**
- **O que Ã©**: onde os dados sÃ£o expostos diretamente para o consumidor final (BI, API, apps).
- **Usos**: dashboards, relatÃ³rios, sistemas externos.
- **Formato**: otimizado para consultas rÃ¡pidas.
- **Exemplos**:
  - Power BI, Tableau (conectados ao DW)
  - APIs com dados consolidados
  - Dashboards internos

##### ğŸ—ï¸ Arquitetura de exemplo com armazenamento:

```text
[Arquivo CSV] â†’ S3 (raw)
           â†“
 TransformaÃ§Ã£o â†’ Tabela staging no PostgreSQL
           â†“
 Enriquecimento â†’ Tabela final no Data Warehouse
           â†“
 BI â†’ Power BI conectado ao DW
```

##### ğŸ“Œ ConclusÃ£o:

O **armazenamento** Ã© essencial em cada etapa:
- **Antes da transformaÃ§Ã£o**: guardar o dado cru.
- **Durante**: salvar temporariamente os dados sendo processados.
- **Depois**: guardar os dados confiÃ¡veis, prontos para consumo.

Ele garante **resiliÃªncia, rastreabilidade, e performance** do pipeline.

---

