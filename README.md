# Fundamentos de Engenharia de Dados
Anota√ß√µes sobre o curso Fundamentos de Engenharia de Dados https://www.datascienceacademy.com.br/course/fundamentos-de-engenharia-de-dados

Engenharia de dados √© a √°rea da tecnologia respons√°vel por **projetar, construir, manter e otimizar sistemas que coletam, armazenam, processam e disponibilizam dados** para an√°lise e uso por outras √°reas, como ci√™ncia de dados, BI (business intelligence) e engenharia de software.

### Em outras palavras:
O engenheiro de dados prepara os dados brutos ‚Äî muitas vezes desorganizados, incompletos ou espalhados em v√°rios lugares ‚Äî e transforma isso em algo **organizado, limpo, acess√≠vel e eficiente para an√°lise**.

---

### Principais responsabilidades:
- **Coletar dados** de diversas fontes (bancos de dados, APIs, arquivos, etc.)
- **Criar pipelines de dados (ETL/ELT)** ‚Äî processos que extraem, transformam e carregam dados para um destino final.
- **Modelar dados** para torn√°-los mais √∫teis e acess√≠veis.
- **Garantir a qualidade dos dados** (dados limpos, consistentes e sem duplicidades).
- **Trabalhar com grandes volumes de dados** (Big Data).
- **Construir e gerenciar data lakes e data warehouses**.
- **Automatizar processos de ingest√£o e transforma√ß√£o de dados**.
- **Implementar pol√≠ticas de seguran√ßa e governan√ßa de dados**.

---

### Tecnologias comuns na engenharia de dados:
- **Linguagens**: Python, SQL, Scala.
- **ETL/ELT**: Apache Airflow, dbt, Talend, Dataflow.
- **Big Data**: Apache Spark, Hadoop.
- **Bancos de dados**: PostgreSQL, MySQL, MongoDB, BigQuery, Snowflake, Redshift.
- **Armazenamento em nuvem**: AWS (S3, Glue), GCP (BigQuery, Cloud Storage), Azure.
- **Ferramentas de orquestra√ß√£o e monitoramento**.

---

### Diferen√ßa para outras √°reas:
| √Årea                | Foco principal                                           |
|---------------------|----------------------------------------------------------|
| **Engenharia de Dados** | Infraestrutura e organiza√ß√£o dos dados                 |
| **Ci√™ncia de Dados**     | An√°lise, predi√ß√£o e cria√ß√£o de modelos com os dados    |
| **BI (Business Intelligence)** | Visualiza√ß√£o de dados para tomada de decis√£o        |
| **Engenharia de Software** | Desenvolvimento de sistemas e aplicativos             |

---

### GUIA DE ESTUDO E APRENDIZAGEM DA DATA SCIENCE ACADEMY 
[E-book](./pdf/49-E-book%20DSA%20_Guia_De_Estudo_Aprendizagem.pdf)

### Bibliografia, Refer√™ncias e Links √öteis
[Links](./pdf/10-BibliografiaCap01.pdf)

---

### Pipeline de dados

Um **pipeline de dados** √© como uma "linha de montagem" que **pega dados brutos de uma ou mais fontes, processa esses dados em etapas definidas e entrega o resultado pronto para ser usado** ‚Äî geralmente em um banco de dados, data lake, data warehouse ou ferramenta de an√°lise.

---

### Analogia simples:
Imagine que os dados s√£o gr√£os de caf√©:

1. **Extra√ß√£o**: pegar os gr√£os da planta√ß√£o (dados brutos).
2. **Transforma√ß√£o**: torrar, moer, filtrar (limpar, organizar, juntar).
3. **Carga**: servir o caf√© na x√≠cara (enviar os dados para o destino final).

Esse processo cont√≠nuo de **extra√ß√£o, transforma√ß√£o e carga** √© conhecido como **ETL (Extract, Transform, Load)** ou **ELT** (quando a transforma√ß√£o vem depois da carga).

---

### Fases de um pipeline de dados:

1. **Extra√ß√£o (Extract)**  
   Captura dados de fontes diversas:
   - APIs
   - Bancos de dados
   - Arquivos CSV, Excel
   - Servi√ßos de terceiros (como Google Analytics, Facebook Ads)

2. **Transforma√ß√£o (Transform)**  
   Prepara os dados:
   - Limpeza (remover duplicatas, preencher valores ausentes)
   - Convers√£o de formatos
   - Jun√ß√£o de diferentes tabelas
   - Aplica√ß√£o de regras de neg√≥cio

3. **Carga (Load)**  
   Envia os dados para o destino:
   - Banco relacional
   - Data warehouse (BigQuery, Redshift, Snowflake)
   - Data lake

---

### Exemplos de ferramentas de pipeline:

- **Apache Airflow** (orquestra√ß√£o de tarefas e agendamentos)
- **dbt** (transforma√ß√µes SQL em data warehouse)
- **Luigi**, **Prefect**, **Kedro** (orquestra√ß√£o)
- **Talend**, **Informatica**, **AWS Glue**, **GCP Dataflow**

---

### Por que usar um pipeline?

- Automatiza o processo de movimenta√ß√£o e tratamento de dados.
- Garante qualidade e consist√™ncia.
- Torna o fluxo de dados escal√°vel e monitor√°vel.
- Facilita a atualiza√ß√£o de dados em tempo real ou agendada.

---

### Exemplo de um pipeline de dados

#### üß† Cen√°rio:
Temos um arquivo `usuarios.csv` com dados de usu√°rios. Vamos:

1. **Extrair** os dados do CSV.  
2. **Transformar**: remover linhas com e-mails inv√°lidos.  
3. **Carregar**: salvar os dados limpos em um banco SQLite.

---

#### üìÅ Exemplo do CSV (`usuarios.csv`):

```csv
id,nome,email
1,Ana,ana@email.com
2,Jo√£o,joao@email.com
3,Lucas,lucas@email
4,Marina,marina@email.com
```

> Note que o email do Lucas est√° inv√°lido (sem ".com").

---

#### üíª C√≥digo do pipeline (`pipeline_etl.py`):

```python
import pandas as pd
import sqlite3
import re

# EXTRA√á√ÉO
def extrair_dados(caminho_csv):
    return pd.read_csv(caminho_csv)

# TRANSFORMA√á√ÉO
def limpar_dados(df):
    # Remove e-mails inv√°lidos com regex simples
    regex_email = r"[^@]+@[^@]+\.[^@]+"
    df_filtrado = df[df['email'].apply(lambda x: re.match(regex_email, x) is not None)]
    return df_filtrado

# CARGA
def carregar_dados(df, nome_banco):
    conn = sqlite3.connect(nome_banco)
    df.to_sql('usuarios', conn, if_exists='replace', index=False)
    conn.close()

# EXECU√á√ÉO DO PIPELINE
def pipeline():
    print("Iniciando pipeline...")
    dados = extrair_dados('usuarios.csv')
    print("Extra√ß√£o conclu√≠da!")

    dados_limpos = limpar_dados(dados)
    print("Transforma√ß√£o conclu√≠da!")

    carregar_dados(dados_limpos, 'dados.db')
    print("Carga conclu√≠da! Dados salvos no banco 'dados.db'.")

if __name__ == "__main__":
    pipeline()
```

---

#### üì¶ Resultado:
- O pipeline cria um banco SQLite (`dados.db`) com uma tabela `usuarios` contendo **somente os registros v√°lidos**.
- O Lucas ser√° exclu√≠do por ter um e-mail inv√°lido.

---

### Componentes de um pipeline de dados

#### ‚úÖ **1. Origem (Source)**  
S√£o os **locais onde os dados se encontram antes de serem processados**.

üîπ **Componentes**:
- Bancos de dados relacionais (MySQL, PostgreSQL, SQL Server)
- Bancos NoSQL (MongoDB, Firebase, Cassandra)
- APIs (Google Ads, Facebook, Stripe, etc.)
- Arquivos (CSV, Excel, JSON, XML)
- Sistemas legados (ERP, CRM, etc.)
- Dados em tempo real (Kafka, IoT)

---

#### ‚öôÔ∏è **2. Processamento (Processing)**  
√â o **cora√ß√£o do pipeline**, onde os dados s√£o extra√≠dos, tratados, organizados e preparados para o uso.

üîπ **Componentes**:

##### A) **Extra√ß√£o (Extract)**  
- Captura os dados da origem.

##### B) **Transforma√ß√£o (Transform)**  
- Limpeza, normaliza√ß√£o, enriquecimento e padroniza√ß√£o dos dados.

##### C) **Orquestra√ß√£o e Agendamento**
- Coordena a ordem e o momento em que cada tarefa roda.
- Ferramentas: Apache Airflow, Prefect, Dagster

##### D) **Valida√ß√£o e Qualidade de Dados**
- Verifica se os dados est√£o consistentes e corretos.
- Regras de neg√≥cio, alertas e checagens.

##### E) **Monitoramento e Logs**
- Acompanha o status das execu√ß√µes e detecta falhas.

---

#### üì¶ **3. Destino (Target)**  
√â onde os dados **processados s√£o armazenados e ficam prontos para uso** por sistemas, BI, machine learning, etc.

üîπ **Componentes**:
- Data Warehouses (BigQuery, Redshift, Snowflake)
- Data Lakes (AWS S3, Azure Data Lake)
- Bancos SQL (PostgreSQL, MySQL)
- NoSQL (MongoDB, Elasticsearch)
- Dashboards de BI (Power BI, Looker, Tableau)
- Modelos de Machine Learning

---

#### üß† Visual Resumido:

```plaintext
[ ORIGEM ]
    ‚Üì
[ EXTRA√á√ÉO ]
    ‚Üì
[ TRANSFORMA√á√ÉO ]
    ‚Üì
[ CARGA ]
    ‚Üì
[ DESTINO ]
    ‚Üì
[ CONSUMO (dashboards, an√°lises, ML, etc.) ]
```

---

### Pipeline de dados x pipeline ETL

Pipeline de dados e pipeline ETL **nem sempre s√£o a mesma coisa**, mas muitas vezes s√£o usados como **sin√¥nimos** ‚Äî especialmente em contextos mais simples.

Vamos ver a diferen√ßa com clareza:

---

#### ‚úÖ **Pipeline ETL** (Extract, Transform, Load)

√â um tipo espec√≠fico de pipeline de dados que segue **tr√™s etapas principais**:

1. **Extract (Extra√ß√£o)** ‚Äì pega dados brutos de uma ou mais fontes.  
2. **Transform (Transforma√ß√£o)** ‚Äì limpa, trata e padroniza os dados.  
3. **Load (Carga)** ‚Äì envia os dados para um destino (ex: data warehouse).

üîπ Muito usado quando o foco est√° em **mover e preparar dados para an√°lises** ou BI.

---

#### üîÑ **Pipeline de Dados** (Data Pipeline)

√â um termo **mais gen√©rico e abrangente**.

Pode incluir:

- Pipelines ETL (ou ELT)
- Pipelines de streaming (dados em tempo real)
- Pipelines de machine learning (com ingest√£o, treino de modelo, deploy)
- Pipelines de replica√ß√£o de dados
- Pipelines de integra√ß√£o cont√≠nua com dados

Ou seja, **todo pipeline ETL √© um pipeline de dados**, mas **nem todo pipeline de dados √© ETL**.

---

#### üí° Exemplo de diferen√ßa:

- **Pipeline ETL**: Extrai dados do MySQL, transforma em pandas, carrega no BigQuery.
- **Pipeline de streaming**: Usa Kafka + Spark para processar dados de sensores em tempo real.
- **Pipeline de ML**: Coleta dados, transforma, treina modelo, gera previs√µes automaticamente.

---

#### üß† Resumindo:
| Termo              | Abrang√™ncia | Finalidade principal           |
|--------------------|-------------|--------------------------------|
| **Pipeline ETL**   | Mais espec√≠fico | Movimenta√ß√£o + tratamento de dados |
| **Pipeline de Dados** | Mais amplo     | Qualquer fluxo automatizado de dados |

---

### Principais ferramentas para construir pipeline de dados

#### üîÅ 1. Transforma√ß√£o de Dados

#### O que √©:
S√£o ferramentas que **tratam, limpam, enriquecem, organizam e preparam os dados** para uso ‚Äî geralmente ap√≥s a extra√ß√£o e antes do carregamento final.

#### Tarefas comuns:
- Padronizar nomes de colunas
- Corrigir valores inconsistentes
- Juntar dados de diferentes fontes
- Agregar m√©tricas (ex: soma de vendas por dia)

#### Ferramentas populares:
| Ferramenta     | Descri√ß√£o breve |
|----------------|------------------|
| **dbt (Data Build Tool)** | Transforma dados usando SQL diretamente no data warehouse. Ideal para times de analytics. |
| **Apache Spark** | Processa grandes volumes de dados em cluster (paralelo), com suporte a batch e streaming. |
| **Pandas (Python)** | Biblioteca poderosa para transformar dados tabulares em notebooks/scripts. √ìtima para prototipa√ß√£o. |
| **Apache Beam** | Framework de transforma√ß√£o com suporte a batch e streaming. Roda em Dataflow (GCP), Spark, Flink, etc. |
| **Airbyte / Fivetran / Talend** | Algumas dessas ferramentas tamb√©m permitem transforma√ß√µes, al√©m da extra√ß√£o/carga. |

---

#### ‚òÅÔ∏è 2. Armazenamento e Cloud Computing

#### O que √©:
S√£o os **locais onde os dados s√£o armazenados**, organizados e disponibilizados ‚Äî muitas vezes em nuvem. Tamb√©m inclui servi√ßos que **escalam automaticamente**, como clusters, servidores e bancos gerenciados.

#### Subdivis√µes:
- **Data warehouses** (an√°lises)
- **Data lakes** (armazenamento bruto)
- **Bancos relacionais/NoSQL**
- **Infraestrutura em nuvem (IaaS/PaaS)**

#### Ferramentas populares:
| Ferramenta     | Descri√ß√£o breve |
|----------------|------------------|
| **Google BigQuery** | Data warehouse serverless da GCP, ideal para grandes volumes e consultas r√°pidas. |
| **Amazon Redshift** | Data warehouse da AWS, otimizado para an√°lises massivas. |
| **Snowflake** | Data warehouse multi-cloud, altamente escal√°vel. |
| **AWS S3** | Armazena arquivos em nuvem (data lake). |
| **Azure Data Lake** | Equivalente ao S3 na Azure. |
| **Databricks** | Plataforma para engenharia e ci√™ncia de dados baseada em Spark. |
| **Google Cloud Platform / AWS / Azure** | Provedores cloud com servi√ßos integrados de dados, computa√ß√£o, seguran√ßa, etc. |

---

#### ‚ö° 3. Real-Time Analytics (An√°lise em Tempo Real)

#### O que √©:
S√£o ferramentas e plataformas voltadas para **ingest√£o, processamento e an√°lise de dados em tempo real ou quase tempo real**. Ideal para sistemas que precisam de respostas imediatas (ex: detec√ß√£o de fraudes, monitoramento de sensores, logs).

#### Ferramentas populares:
| Ferramenta     | Descri√ß√£o breve |
|----------------|------------------|
| **Apache Kafka** | Sistema de mensageria distribu√≠do, ideal para capturar e transmitir eventos em tempo real. |
| **Apache Flink** | Processamento de dados em streaming com baixa lat√™ncia. |
| **Apache Spark Structured Streaming** | M√≥dulo do Spark para trabalhar com dados em tempo real. |
| **Google Dataflow** | Servi√ßo de stream/batch processing na GCP, baseado em Apache Beam. |
| **Kinesis (AWS)** | Equivalente ao Kafka na AWS, ideal para ingest√£o de dados em tempo real. |
| **ClickHouse** | Banco OLAP de alta performance, muito usado para analytics em tempo real. |

---

#### üí° Resumo visual da classifica√ß√£o:

```plaintext
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Transforma√ß√£o de Dados    ‚îÇ  ‚Üê limpeza, preparo, jun√ß√µes
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  dbt, Spark, Pandas, Beam  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Armazenamento e Cloud       ‚îÇ  ‚Üê onde os dados s√£o guardados
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  BigQuery, Redshift, S3,     ‚îÇ
‚îÇ  Snowflake, Databricks       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Real-Time Analytics       ‚îÇ  ‚Üê dados em tempo real
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Kafka, Flink, Dataflow,   ‚îÇ
‚îÇ  Spark Streaming, Kinesis  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### O processo de engenharia de dados

* A engenharia de dados fornece o suporte necess√°rio para que o processo de Ci√™ncia de Dados possa ser executado
* A Engenharia de Dados n√£o participa de todo o processo de Ci√™ncia de Dados, mas sim das atividades onde deve haver gest√£o de dados
* Uma das primeiras etapas do processo √© a extra√ß√£o de dados de uma ou mais fonts, de dados em batch ou dados em streaming
* A etapa seguinte envolve a prepara√ß√£o dos dados com atividades de limpeza, transforma√ß√£o, enriquecimento e seguran√ßa de acesso
* O processo continua com o armazenamento dos dados no destino ou uso em tempo real 
* Mas o que seria o uso dos dados? An√°lise, gr√°ficos, dashboards, machine learning, IA ou qualquer outra tarefa de Ci√™ncia de dados
* O pipeline de dados pode ser executado uma √∫nica vez ou diversas vezes
* A execu√ß√£o do pipeline requer monitoramento, seguran√ßa, valida√ß√£o e documenta√ß√£o
* O pipeline de dados √© ent√£o criado, documentado e automatizado
* E ent√£o outro processo ser√° iniciado para outro produto de dados da empresa
* Um Arquiteto de Dados √© o respons√°vel pro projetar a solu√ß√£o
* O trabalho de um Engenheiro de Dados envolve construir e mantes a solu√ß√£o de dados

---

### Ciclo de vida da engenharia de dados

![Diagrama ciclo de vida da engenharia de dados](./img/ciclo-de-vida-engenhraria-de-dados.png)

#### Fonte de dados (batch e streaming)

As **fontes de dados** podem ser classificadas com base em **como os dados s√£o coletados e processados**, e os dois principais modos s√£o:

#### 1. **Batch (Lote)**

- **O que √©**: Os dados s√£o coletados, armazenados por um tempo e processados **em blocos (lotes)** em intervalos definidos (ex: a cada hora, dia, semana).
- **Quando usar**: Quando os dados n√£o precisam ser atualizados em tempo real e podem ser processados periodicamente.
- **Exemplos**:
  - Relat√≥rios financeiros gerados toda madrugada.
  - Importa√ß√£o de dados de um CRM (como Salesforce) uma vez por dia.
  - ETL noturno que processa arquivos CSV enviados por e-mail.
- **Ferramentas comuns**: Apache Airflow, AWS Glue, Talend, dbt, Spark (modo batch).

##### ‚úÖ Vantagens:
- Mais simples de implementar e manter.
- Maior performance para grandes volumes.

##### ‚ùå Desvantagens:
- Dados n√£o est√£o atualizados em tempo real.
- Pode haver lat√™ncia alta (minutos a horas).

#### 2. **Streaming (Tempo real)**

- **O que √©**: Os dados s√£o processados **em tempo real ou quase em tempo real**, assim que s√£o gerados.
- **Quando usar**: Quando √© necess√°rio agir rapidamente com base nos dados mais recentes.
- **Exemplos**:
  - Detec√ß√£o de fraudes banc√°rias com base em transa√ß√µes em tempo real.
  - Leituras de sensores IoT transmitidas continuamente.
  - Cliques em an√∫ncios ou navega√ß√£o de usu√°rios em um site.
- **Ferramentas comuns**: Apache Kafka, Apache Flink, Spark Streaming, AWS Kinesis, Google Pub/Sub.

##### ‚úÖ Vantagens:
- Rea√ß√µes r√°pidas a eventos.
- Ideal para sistemas cr√≠ticos ou interativos.

##### ‚ùå Desvantagens:
- Mais complexo de desenvolver e monitorar.
- Pode ter maior custo operacional.

#### 3. **Lambda Architecture (H√≠brido: Batch + Streaming)**

- **O que √©**: Combina o melhor dos dois mundos: uma camada de batch para consist√™ncia e uma de streaming para baixa lat√™ncia.
- **Exemplo**:
  - Dados de pedidos de e-commerce processados em tempo real para atualiza√ß√£o de estoque, mas reprocessados √† noite em batch para gerar relat√≥rios consolidados.

#### Comparativo r√°pido:

| Caracter√≠stica     | Batch                          | Streaming                       |
|--------------------|--------------------------------|----------------------------------|
| Lat√™ncia            | Alta (minutos a horas)         | Baixa (milissegundos a segundos)|
| Volume              | Muito alto de uma vez          | Cont√≠nuo e constante             |
| Complexidade        | Menor                          | Maior                            |
| Casos ideais        | BI, relat√≥rios, ETL            | Alertas, logs, sistemas online   |

---

#### Ingest√£o de dados

**Ingest√£o de dados** √© o processo de **coletar dados de uma ou mais fontes** e **traz√™-los para dentro de um sistema de armazenamento ou processamento**, como um data lake, data warehouse, banco de dados ou pipeline.

##### üîç Explicando de forma simples:

Imagine que as fontes de dados s√£o como **torneiras espalhadas pela empresa** (sites, bancos de dados, APIs, arquivos, sensores, etc.).  
A ingest√£o de dados √© o **encanamento que conecta essas torneiras ao seu reservat√≥rio de dados**, garantindo que a √°gua (dados) chegue limpa, no hor√°rio certo e no volume certo.

##### Tipos de Ingest√£o de Dados:

1. **Batch (lote)**  
   - Dados s√£o coletados em grandes blocos, de tempos em tempos.  
   - Ex: Buscar um arquivo CSV por FTP toda noite √†s 2h da manh√£.

2. **Streaming (tempo real)**  
   - Dados s√£o ingeridos continuamente, conforme s√£o gerados.  
   - Ex: Processar eventos do Kafka conforme chegam.

3. **Micro-batch**  
   - Um meio-termo: dados s√£o agrupados em pequenos lotes com pouca lat√™ncia.  
   - Ex: Um job do Spark rodando a cada 30 segundos.

##### Fontes comuns de ingest√£o:

- Bancos de dados (MySQL, PostgreSQL, Oracle)
- APIs REST
- Arquivos CSV, Excel, JSON, Parquet
- Sistemas ERP ou CRM
- Fila de mensagens (Kafka, RabbitMQ)
- Sensores e dispositivos IoT
- Servi√ßos de nuvem (Google Analytics, Facebook Ads, etc.)

##### Ferramentas populares para ingest√£o:

- **Batch**: Apache Nifi, Apache Sqoop, Talend, Airbyte  
- **Streaming**: Apache Kafka, Flink, Logstash, Kinesis  
- **Cloud**: AWS Glue, Google Cloud Dataflow, Azure Data Factory  

##### Exemplos pr√°ticos:

- Ingerir logs de acesso de um site para an√°lise de tr√°fego.
- Puxar diariamente os leads do RD Station para um banco.
- Receber dados de temperatura de sensores industriais em tempo real.

##### Por que a ingest√£o √© t√£o importante?

Porque **sem ingest√£o, n√£o h√° dado para trabalhar**. √â o primeiro passo para:
- Criar dashboards.
- Treinar modelos de machine learning.
- Fazer an√°lises estat√≠sticas.
- Automatizar decis√µes baseadas em dados.

---

#### Transforma√ß√£o e Enriquecimento

Depois da **ingest√£o**, os dados raramente est√£o prontos para uso. √â a√≠ que entram a **transforma√ß√£o** e o **enriquecimento** dos dados ‚Äî duas etapas fundamentais na engenharia de dados.

##### üîß **Transforma√ß√£o de Dados**

**Transformar dados** significa alterar sua estrutura ou formato para que fiquem **mais limpos, padronizados, √∫teis e compat√≠veis com os objetivos do neg√≥cio**.

##### Exemplos de transforma√ß√£o:
- Converter datas para um √∫nico formato (ex: `01/04/2025` ‚Üí `2025-04-01`)
- Trocar v√≠rgulas por pontos em n√∫meros decimais (`12,3` ‚Üí `12.3`)
- Remover espa√ßos em branco ou caracteres especiais
- Agrupar dados (ex: somar vendas por m√™s)
- Normalizar textos (`S√£o Paulo`, `sao paulo`, `S√ÉO PAULO` ‚Üí `Sao Paulo`)
- Criar colunas novas a partir de outras (ex: extrair o **ano** de uma data)

##### Ferramentas e t√©cnicas:
- SQL (com `SELECT`, `CASE`, `CAST`, `JOIN`, etc.)
- Linguagens como Python (pandas), Scala, R
- Ferramentas ETL: dbt, Apache Spark, Airflow, etc.

##### ‚ú® **Enriquecimento de Dados**

**Enriquecer dados** √© o processo de **adicionar novas informa√ß√µes a um conjunto de dados existente** para torn√°-lo mais completo, √∫til e valioso.

##### Exemplos de enriquecimento:
- Juntar dados de clientes com dados geogr√°ficos (CEP ‚Üí cidade e estado)
- Incluir a cota√ß√£o do d√≥lar em uma tabela de exporta√ß√µes
- Associar um ID de produto com sua descri√ß√£o e categoria
- Usar uma API externa para descobrir a localiza√ß√£o exata de um IP

#### Fontes para enriquecimento:
- Outras tabelas internas (via `JOIN`)
- APIs externas (Google Maps, Receita Federal, redes sociais)
- Cat√°logos de dados p√∫blicos

##### Resumindo:

| Etapa           | O que faz                                 | Exemplo pr√°tico                            |
|----------------|--------------------------------------------|---------------------------------------------|
| **Transforma√ß√£o** | Limpa e organiza os dados                 | Corrigir formata√ß√£o de datas, normalizar texto |
| **Enriquecimento** | Adiciona novas informa√ß√µes √∫teis          | Trazer o nome do cliente a partir de um ID  |

Ambas as etapas s√£o parte do famoso processo **ETL (Extract, Transform, Load)**, e s√£o essenciais para garantir que os dados estejam **confi√°veis, completos e prontos para an√°lise** ou machine learning.

---

#### Carga e Uso dos Dados

Depois de **ingest√£o**, **transforma√ß√£o** e **enriquecimento**, o ciclo √© fechado com as fases finais do pipeline de engenharia de dados: **Carga (Load)** e **Uso dos Dados**.

##### üöö **Carga (Load)**

A **carga de dados** √© o processo de **salvar os dados transformados e enriquecidos no destino final**, que pode ser:

- Um **Data Lake** (armazenamento bruto em grande escala, ex: S3, Google Cloud Storage)
- Um **Data Warehouse** (estrutura otimizada para an√°lises, ex: BigQuery, Redshift, Snowflake)
- Um **banco de dados relacional** (ex: PostgreSQL, MySQL, SQL Server)
- Uma ferramenta de **BI** (ex: Power BI, Tableau)
- Um sistema de terceiros (ex: CRM, ERP, API externa)

##### Tipos de carga:
- **Full Load (carga total)**: os dados antigos s√£o apagados e substitu√≠dos por uma vers√£o atualizada.
- **Incremental Load (carga incremental)**: apenas os dados novos ou modificados s√£o adicionados/atualizados.

##### Exemplo:
Ap√≥s transformar uma planilha de vendas, os dados s√£o carregados para um banco PostgreSQL, onde os analistas poder√£o consult√°-los.

##### üìä **Uso dos Dados**

Depois da carga, os dados est√£o prontos para serem **consumidos**. Isso √© onde eles finalmente **geram valor para o neg√≥cio**.

##### Principais formas de uso:

1. **Business Intelligence (BI)**  
   - Ferramentas como Power BI, Tableau e Looker acessam os dados e criam dashboards, KPIs e relat√≥rios interativos.
   - Ex: visualizar vendas por regi√£o e por per√≠odo.

2. **Data Science / Machine Learning**  
   - Cientistas de dados usam os dados carregados para treinar modelos, prever comportamentos ou detectar anomalias.
   - Ex: prever churn de clientes ou demanda futura.

3. **APIs e produtos de dados**  
   - Os dados podem alimentar APIs internas ou produtos digitais (apps, sistemas, etc.).
   - Ex: uma API de recomenda√ß√£o de produtos baseada em dados de comportamento.

4. **Tarefas operacionais / automa√ß√µes**  
   - Rob√¥s, scripts ou sistemas usam os dados para tomar decis√µes ou realizar a√ß√µes automaticamente.
   - Ex: disparar um e-mail de alerta quando um estoque estiver abaixo do m√≠nimo.

##### üîÑ Onde isso entra no fluxo geral?

```text
[Ingest√£o] ‚Üí [Transforma√ß√£o] ‚Üí [Enriquecimento] ‚Üí [Carga] ‚Üí [Uso dos Dados]
```

Cada etapa √© dependente da anterior, e todas fazem parte do **pipeline de dados**.

#### Exemplo pr√°tico

**Mini pipeline de engenharia de dados**, simulando um cen√°rio real simples ‚Äî passo a passo ‚Äî desde a ingest√£o at√© o uso.

##### üß™ **Cen√°rio fict√≠cio: An√°lise de vendas de uma loja online**

Voc√™ trabalha como engenheiro de dados e precisa entregar dados de vendas di√°rias limpos e prontos para o time de BI gerar relat√≥rios.

###### ‚úÖ 1. **Ingest√£o**

**Fonte de dados**:  
Um sistema da loja exporta diariamente um arquivo CSV com as vendas do dia, como este:

```
id,produto_id,data_venda,preco,quantidade
1,101,27/04/2025,49,90,1
2,102,27/04/2025,129.99,2
```

Voc√™ cria um script que **l√™ esse arquivo todo dia √†s 23h** e o envia para o pipeline.

Ferramentas poss√≠veis: Python (`pandas`), Apache Airflow (agendamento), Shell Script.

##### üîß 2. **Transforma√ß√£o**

Voc√™ nota problemas:
- O campo `data_venda` est√° no formato errado.
- O campo `preco` vem com v√≠rgula ao inv√©s de ponto (ex: `49,90`).
- Precisa de uma nova coluna: **valor_total** (pre√ßo √ó quantidade)

Voc√™ transforma os dados assim:

```python
import pandas as pd

df = pd.read_csv("vendas.csv", sep=",")
df['data_venda'] = pd.to_datetime(df['data_venda'], dayfirst=True)
df['preco'] = df['preco'].astype(str).str.replace(',', '.').astype(float)
df['valor_total'] = df['preco'] * df['quantidade']
```

##### ‚ú® 3. **Enriquecimento**

Voc√™ tem uma outra tabela de produtos:

```csv
produto_id,nome,categoria
101,Camisa Polo,Roupas
102,Smartwatch,Eletr√¥nicos
```

Voc√™ faz um **JOIN** para adicionar nome e categoria do produto:

```python
produtos = pd.read_csv("produtos.csv")
df = df.merge(produtos, on='produto_id', how='left')
```

Agora seu dataframe tem:

```
id | produto_id | data_venda | preco | quantidade | valor_total | nome         | categoria
1  | 101        | 2025-04-27 | 49.90 | 1          | 49.90       | Camisa Polo  | Roupas
```

##### üöö 4. **Carga**

Voc√™ salva os dados prontos no seu **Data Warehouse** (ex: PostgreSQL ou BigQuery):

```python
from sqlalchemy import create_engine

engine = create_engine("postgresql://user:senha@host:porta/banco")
df.to_sql("vendas_processadas", engine, if_exists="append", index=False)
```

##### üìä 5. **Uso dos Dados**

Agora, o time de BI conecta o Power BI diretamente na tabela `vendas_processadas`, e pode:

- Ver **vendas por categoria**
- Comparar **faturamento por per√≠odo**
- Criar **gr√°ficos e dashboards**

##### üîÑ Resumo visual:

```text
üìÅ CSV de vendas (raw)
 ‚Üì ingest√£o
üßπ Limpeza + C√°lculo de total
 ‚Üì transforma√ß√£o
‚ûï Adi√ß√£o de nome e categoria do produto
 ‚Üì enriquecimento
üíæ Salva no PostgreSQL
 ‚Üì carga
üìä Usado no Power BI para relat√≥rios
```

---

#### Armazenamento

Vamos entender **onde entra o armazenamento** no pipeline de dados e **quais tipos existem**, com foco na etapa de **carga** e posterior **uso dos dados**.

##### üóÑÔ∏è **Armazenamento no pipeline de dados**

O **armazenamento** √© onde os dados ficam **guardados em cada etapa** do processo ‚Äî desde os brutos at√© os prontos para an√°lise.

Ele pode ocorrer em **v√°rias camadas diferentes**, dependendo da arquitetura adotada. Veja abaixo:

##### üß± Tipos de armazenamento no pipeline:

##### 1. **Raw Layer (Camada Bruta)**
- **O que √©**: onde os dados s√£o armazenados *logo ap√≥s a ingest√£o*, sem transforma√ß√£o.
- **Usos**: backup, reprocessamento, auditoria.
- **Formato**: arquivos CSV, JSON, Parquet, Avro.
- **Exemplos**:
  - Amazon S3
  - Google Cloud Storage
  - Azure Blob Storage
  - HDFS (Hadoop)

##### 2. **Staging Layer (Camada Intermedi√°ria ou Tempor√°ria)**
- **O que √©**: onde os dados **transformados parcialmente** ficam antes do armazenamento final.
- **Usos**: etapa de prepara√ß√£o, controle de qualidade, testes.
- **Formato**: tabelas tempor√°rias, arquivos organizados.
- **Exemplos**:
  - Tabelas staging no PostgreSQL, BigQuery, etc.
  - Pastas intermedi√°rias no S3

##### 3. **Curated / Trusted Layer (Camada Curada ou Confi√°vel)**
- **O que √©**: onde os dados **prontos para an√°lise** s√£o armazenados.
- **Usos**: BI, relat√≥rios, ML, dashboards.
- **Formato**: tabelas bem definidas, padronizadas.
- **Exemplos**:
  - Data Warehouse (Redshift, BigQuery, Snowflake, PostgreSQL)
  - Data Mart (subconjunto voltado para √°reas espec√≠ficas)

##### 4. **Serving Layer (Camada de Consumo)**
- **O que √©**: onde os dados s√£o expostos diretamente para o consumidor final (BI, API, apps).
- **Usos**: dashboards, relat√≥rios, sistemas externos.
- **Formato**: otimizado para consultas r√°pidas.
- **Exemplos**:
  - Power BI, Tableau (conectados ao DW)
  - APIs com dados consolidados
  - Dashboards internos

##### üèóÔ∏è Arquitetura de exemplo com armazenamento:

```text
[Arquivo CSV] ‚Üí S3 (raw)
           ‚Üì
 Transforma√ß√£o ‚Üí Tabela staging no PostgreSQL
           ‚Üì
 Enriquecimento ‚Üí Tabela final no Data Warehouse
           ‚Üì
 BI ‚Üí Power BI conectado ao DW
```

##### üìå Conclus√£o:

O **armazenamento** √© essencial em cada etapa:
- **Antes da transforma√ß√£o**: guardar o dado cru.
- **Durante**: salvar temporariamente os dados sendo processados.
- **Depois**: guardar os dados confi√°veis, prontos para consumo.

Ele garante **resili√™ncia, rastreabilidade, e performance** do pipeline.

---

